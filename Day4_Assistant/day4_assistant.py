# -*- coding: utf-8 -*-
"""Day4_Assistant.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i104BYU7iCrWFyfXErR3p4p30bwurIFk

# Day 4 – Conversational Memory & Integrated Assistant

## Objective
This notebook implements a multi-turn conversational assistant that combines:

- Conversational memory
- Retrieval-Augmented Generation (RAG)
- Agent-style tool usage

The assistant remembers previous interactions, retrieves relevant context, and generates grounded responses.
"""

!pip install -U langchain langchain-core langchain-community langchain-text-splitters faiss-cpu sentence-transformers transformers

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.tools import Tool
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

pipe = pipeline(
    "text-generation",
    model="google/flan-t5-base",
    max_new_tokens=200
)

llm = HuggingFacePipeline(pipeline=pipe)

text_data = """
Artificial Intelligence enables machines to mimic human intelligence.
RAG improves responses by retrieving relevant context.
Agents can use tools to improve reliability.
Memory allows assistants to maintain conversation history.
"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)

docs = splitter.create_documents([text_data])

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = FAISS.from_documents(docs, embeddings)
retriever = vectorstore.as_retriever()

def calculator_tool(expression):
    return str(eval(expression))

calculator = Tool(
    name="Calculator",
    func=calculator_tool,
    description="Useful for solving math expressions"
)

memory = []

def assistant(query):

    print("Assistant reasoning started...")

    # Store user input
    memory.append({"user": query})

    # ----- Retrieval -----
    retrieved_docs = retriever.invoke(query)
    context = "\n".join([doc.page_content for doc in retrieved_docs])

    # ----- Agent-like tool selection -----
    if any(char.isdigit() for char in query):
        print("Tool used: Calculator")
        expression = query.replace("What is", "").replace("?", "")
        tool_result = calculator.run(expression)
    else:
        tool_result = "No tool used"

    # ----- Build final prompt -----
    prompt = f"""
Context:
{context}

Conversation History:
{memory}

Tool Output:
{tool_result}

User Question:
{query}

Answer using context, memory, and tool output.
"""

    answer = llm.invoke(prompt)

    # Store assistant response
    memory.append({"assistant": answer})

    return answer

print(assistant("What is RAG?"))

print(assistant("Why is memory useful in assistants?"))

print(assistant("What is 25 * 12 + 8?"))

"""## Integrated Workflow

This assistant combines three core components:

1. Retrieval (RAG)
   - Retrieves relevant document context.

2. Conversational Memory
   - Stores previous user and assistant messages.

3. Agent Behavior
   - Chooses whether to invoke a tool.

Workflow:

User Query  
↓  
Retrieve Context  
↓  
Check Memory  
↓  
Tool Decision  
↓  
Generate Final Response
"""