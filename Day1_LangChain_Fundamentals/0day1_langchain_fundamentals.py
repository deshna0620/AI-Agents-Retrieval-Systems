# -*- coding: utf-8 -*-
"""0Day1_LangChain_Fundamentals.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vyJ3ibmq5nK5dSjCmW2edGKkPyIL8ovw

# Day 1 – LangChain Fundamentals

## Objective
This notebook demonstrates the basic workflow used in LLM-based applications using LangChain.

### Goals
- Create prompt templates
- Build structured LLM chains
- Demonstrate input formatting
- Perform output parsing
- Show different prompt behaviors
- Explain modular workflow design
"""

!pip install -U langchain langchain-core langchain-community transformers

from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline

pipe = pipeline(
    "text-generation",
    model="google/flan-t5-base",
    max_new_tokens=200
)

llm = HuggingFacePipeline(pipeline=pipe)

"""## LangChain Chain Workflow

A LangChain workflow follows this structure:

Input → Prompt Template → LLM → Output Parser → Final Output

Each component has a specific responsibility:
- Prompt Template: Formats input dynamically
- LLM: Generates the response
- Output Parser: Cleans and formats output

This modular design makes workflows reusable and scalable.
"""

prompt = PromptTemplate(
    input_variables=["topic", "style"],
    template="Explain {topic} in {style} style."
)

formatted_prompt = prompt.format(
    topic="Machine Learning",
    style="simple beginner"
)

print(formatted_prompt)

output_parser = StrOutputParser()

chain = prompt | llm | output_parser

response = chain.invoke({
    "topic": "Neural Networks",
    "style": "easy explanation"
})

print(response)

explain_prompt = PromptTemplate(
    input_variables=["concept"],
    template="Explain {concept} for a beginner college student."
)

explain_chain = explain_prompt | llm | StrOutputParser()

print(explain_chain.invoke({"concept": "AI Agents"}))

summary_prompt = PromptTemplate(
    input_variables=["text"],
    template="Summarize the following in 3 bullet points:\n{text}"
)

summary_chain = summary_prompt | llm | StrOutputParser()

print(summary_chain.invoke({
    "text": "Artificial intelligence systems perform reasoning, learning and decision-making using data-driven models."
}))

"""## How Chains Improve Workflow Modularity

LangChain separates the workflow into independent components:

- Prompt Template
- Language Model
- Output Parser

Advantages:
- Reusability of components
- Easier debugging
- Cleaner architecture
- Easy model replacement

Changing one component does not break the entire system.

## Conclusion

This notebook demonstrates:

- Prompt templates
- Structured LLM chains
- Input formatting
- Output parsing
- Multiple prompt behaviors
- Modular workflow design

This chain-based architecture forms the foundation for:
- Retrieval-Augmented Generation (RAG)
- AI Agents
- Conversational Assistants
"""