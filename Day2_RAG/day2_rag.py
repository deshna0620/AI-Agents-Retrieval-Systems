# -*- coding: utf-8 -*-
"""Day2_RAG.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r9qtXsl6Wotj95g0iumkVgpkPjZCahbV

# Day 2 – Retrieval-Augmented Generation (RAG)

## Objective
The objective of this notebook is to implement a Retrieval-Augmented Generation (RAG) pipeline using LangChain.  
RAG improves answer quality by retrieving relevant information from external documents before generating a response using an LLM.

---

## Concepts Covered
- Document loading
- Text chunking
- Embeddings generation
- Vector database (FAISS)
- Similarity-based retrieval
- Grounded response generation

---

## What is RAG?
Retrieval-Augmented Generation (RAG) is a technique that combines:

1. **Retrieval System** – searches for relevant information from a document collection.
2. **Language Model (LLM)** – generates answers using retrieved context.

Instead of relying only on model memory, the system first retrieves relevant text and then generates a grounded answer.

---

## Workflow Architecture

User Query  
↓  
Retriever (Vector Database – FAISS)  
↓  
Relevant Document Chunks  
↓  
Prompt Template  
↓  
LLM  
↓  
Grounded Answer

---

## Expected Outcome
This notebook demonstrates:
- Loading and processing text data
- Building embeddings and vector index
- Performing similarity search
- Generating answers strictly using retrieved context

The final system shows a working Retrieval-Augmented Generation pipeline.
"""

!pip install -U langchain langchain-core langchain-community langchain-text-splitters faiss-cpu sentence-transformers transformers

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.llms import HuggingFacePipeline
from transformers import pipeline

pipe = pipeline(
    "text-generation",
    model="google/flan-t5-base",
    max_new_tokens=200
)

llm = HuggingFacePipeline(pipeline=pipe)

text_data = """
Artificial Intelligence (AI) enables machines to mimic human intelligence.
Machine Learning is a subset of AI that learns from data.
Deep Learning uses neural networks with multiple layers.
RAG stands for Retrieval-Augmented Generation.
RAG improves accuracy by retrieving relevant external information before generating answers.
"""

splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20
)

docs = splitter.create_documents([text_data])

print(docs)

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = FAISS.from_documents(docs, embeddings)

retriever = vectorstore.as_retriever()

query = "What is RAG?"

retrieved_docs = retriever.invoke(query)

print(retrieved_docs)

rag_prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Answer ONLY using the context below.

Context:
{context}

Question:
{question}
"""
)

output_parser = StrOutputParser()

rag_chain = rag_prompt | llm | output_parser

context_text = "\n".join([doc.page_content for doc in retrieved_docs])

response = rag_chain.invoke({
    "context": context_text,
    "question": query
})

print(response)

"""## Retrieval Pipeline Explanation

1. Document is loaded.
2. Text is split into smaller chunks.
3. Embeddings convert text into vectors.
4. FAISS stores vectors for fast similarity search.
5. Retriever finds relevant chunks.
6. LLM generates answers using retrieved context.

This ensures grounded and accurate responses.
"""